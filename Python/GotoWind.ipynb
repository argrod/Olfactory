{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 232,
      "metadata": {},
      "outputs": [],
      "source": [
        "import scipy as sp\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from mpmath import besseli\n",
        "import utm\n",
        "from statistics import mode\n",
        "import torch\n",
        "import os, re, glob, pyproj, math, datetime\n",
        "from sys import platform"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Goto wind estimation\n",
        "\n",
        "Originating in the [2016 paper](https://www.science.org/doi/10.1126/sciadv.1700097), this method was originally written for R. Here, we will translate it to work in Python, and try to improve run speeds as well. Comparisons will be made across methods to ensure differences are not being produced."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 236,
      "metadata": {},
      "outputs": [],
      "source": [
        "samplingInterval = 60\n",
        "timeWindow = 51\n",
        "cutlength = 45\n",
        "cutv = 4.1667\n",
        "constv = 34.7/3.6\n",
        "\n",
        "def Likelihoodww(data1,data2,cv): # calculate log-likelihood of the model\n",
        "    def f(par):\n",
        "        a = par[1]\n",
        "        b = cv/sp.gamma(1+1/a)\n",
        "        mx = par[2]\n",
        "        my = par[3]\n",
        "        wx = par[4]\n",
        "        wy = par[5]\n",
        "        L = 0\n",
        "        for i in range(len(data1)):\n",
        "            rr = np.sqrt((data1[i]*np.cos(data2[i]) - wx)**2 + (data1[i]*np.sin(data2[i]) - wy)**2)\n",
        "            rx = (data1[i]*np.cos(data2[i])-wx)/rr\n",
        "            ry = (data1[i]*np.sin(data2[i])-wy)/rr\n",
        "            lp = (a-2)*math.log(rr) - (rr/b)**a + mx*rx + my*ry + math.log(a) - math.log(b) + (1-a)*math.log(b) - math.log(besseli(np.sqrt(mx**2 + my**2),0,))\n",
        "            L = L+lp\n",
        "        return L\n",
        "\n",
        "def Weibull_sd(a,b): # standard deviation of Weibull distribution\n",
        "    return b*np.sqrt(sp.gamma(1+2/a) - sp.gamma(1+1/a)*sp.gamma(1+1/a))\n",
        "\n",
        "def Weibull_mean(a,b): # mean of Weibull distribution\n",
        "    return b*sp.gamma(1+1/a)\n",
        "\n",
        "def Von_Mises_sd(kappa): # standard deviation of von Mises distribution\n",
        "    return 1/np.sqrt(kappa)\n",
        "\n",
        "def readAxyGPS(filename): # read in AxyTrek GPS data (txt files)\n",
        "    df = pd.read_csv(filename, sep = \"\\t\", header = None, usecols = [0,1,2,3],\n",
        "    names = ['Date','Time','lat','lon'])\n",
        "    df['DT'] = pd.to_datetime(df['Date'] + \" \" + df['Time'],format=\"%d/%m/%Y %H:%M:%S\")\n",
        "    return df\n",
        "\n",
        "def readBIPAxy(filename): # read in AxyTrek data as formatted by BIP system\n",
        "    df = pd.read_csv(filename, sep = \",\", header = 0, usecols = [0,1,2], names = ['DT','lat','lon']).dropna()\n",
        "    df['DT'] = pd.to_datetime(df['DT'].str[0:-6], format = \"%Y-%m-%d %H:%M:%S\")\n",
        "    return df\n",
        "\n",
        "def nearest(items, pivot): # find the nearest time position\n",
        "    return min(items, key=lambda x: abs(x - pivot))\n",
        "\n",
        "def timeRescale(dat,tdiff): # calculated indeces for rescaling time (DT) for regular sampling every tdiff mins\n",
        "    return dat.iloc[np.arange(0,len(dat),step=np.timedelta64(tdiff,'m')/np.timedelta64(mode(np.diff(dat['DT'])),'s')).astype(int),:].reset_index()\n",
        "\n",
        "def spTrav(DT,lat,lon,threshold=0): # speed from time (DT), lat, and lon\n",
        "    geod = pyproj.Geod(ellps='WGS84')\n",
        "    _, _, distance = geod.inv(lon.iloc[:-1],lat[:-1],lon[1:],lat[1:])\n",
        "    speed = (distance*10**-3)/np.array(np.diff(DT)/np.timedelta64(3600,'s'))\n",
        "    if threshold != 0:\n",
        "        while np.nanmax(speed) > threshold:\n",
        "            lat = lat[speed < threshold]\n",
        "            lon = lon[speed < threshold]\n",
        "            _, _, distance = geod.inv(lon[:-1],lat[:-1],lon[1:],lat[1:])\n",
        "            speed = (distance)/np.array(np.diff(DT)/np.timedelta64(1,'s'))\n",
        "    return np.append(np.nan,distance), np.append(np.nan,speed)\n",
        "\n",
        "def XYfromUTM(lat,lon): # extract UTM data for lat,lon\n",
        "    return utm.from_latlon(lat,lon)\n",
        "\n",
        "def prePare(filename, convertToMin: bool = True): # prepare BIP data as per required for Goto original method. Adds columns 'dt' (elapsed time from fix to previous time point in seconds), 'dist' (distance travelled from previous point in m), 'track_speed' (in m/sec), 'track_direction' in rad\n",
        "    df = readBIPAxy(filename)\n",
        "    if convertToMin:\n",
        "        df = timeRescale(df,1)\n",
        "    df['dt'] = np.append(np.nan,(np.diff(df['DT']) / np.timedelta64(1,'s')).astype(int))\n",
        "    X,Y,_,_ = XYfromUTM(np.array(df['lat']),np.array(df['lon']))\n",
        "    vg_x_obs = np.diff(X)\n",
        "    vg_y_obs = np.diff(Y)\n",
        "    df['dist'], df['track_speed'] = spTrav(df['DT'],df['lat'],df['lon'])\n",
        "    df['track_direction'] = np.append(np.nan,[math.atan2(vg_y_obs[x],vg_x_obs[x]) for x in range(len(vg_x_obs))])\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# if platform == \"darwin\":\n",
        "#     fileloc = \"/Volumes/GoogleDrive-112399531131798335686/My Drive/PhD/Data/2018Shearwater/AxyTrek/\"\n",
        "# else:\n",
        "#     fileloc = \"I:/My Drive/PD/Data/2018Shearwater/AxyTrek/\"\n",
        "# # list all files\n",
        "# files = glob.glob(fileloc + \"**/*.txt\")\n",
        "# tags = np.unique([re.search('(AxyTrek[\\\\\\\\|/][0-9\\-]+)[\\\\\\\\|/]',f).group(1) for f in files])\n",
        "# dat = readAxyGPS(files[1]) # read in\n",
        "# minDat = timeRescale(dat,1).reset_index() # convert to 1 min fs\n",
        "# dt = (np.diff(minDat['DT']) / np.timedelta64(1,'s')).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 238,
      "metadata": {},
      "outputs": [],
      "source": [
        "# reread data as sample from BIP system\n",
        "if platform == \"darwin\":\n",
        "    filename = \"/Volumes/GoogleDrive-102199952889875375671/My Drive/PD/Data/TestingData/SampleAxyTrek.csv\"\n",
        "else:\n",
        "    filename = \"I:/My Drive/PD/Data/TestingData/SampleAxyTrek.csv\"\n",
        "df = prePare(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Estimation system\n",
        "\n",
        "Once the data are read in, the initial portion of the program deals with idnetifying suitable windows to run the estimation model. These windows are required to by 51 minutes (approximately) in length, and within those 51 minutes, have over 45 samples, assuming a sampling frequency of 1 fix per minute. We assume there to be some error in the sampling interval. In the original study, this was taken as 5 seconds (i.e. we can expect samples to be 60 $\\pm$ 5 seconds).\n",
        "\n",
        "Starting from the first possible startpoint (half the window size in samples), the model then runs through the following processes:\n",
        "\n",
        "1. Define window size.\n",
        "   1. Find position of data which is 25.5 minutes after initial point.\n",
        "   2. Repeat but for before initial point.\n",
        "   3. Assign these positions as start and end of the window.\n",
        "2. Create a new vector of track speed and direction where the speed is above the threshold of 4.1667 m/s, the sample is within 65 s of the previous sample, and direction is not equal to 100.\n",
        "\n",
        "At this stage, the model starts to run through a variety of 'initial headings', set as each integer between -3 and 3. For each initial heading, the following processes are run:\n",
        "\n",
        "1. Create variable `inita` as a random variable generated from a normal distribution with mean 12.5 and standard deviation 5. `inita` must be greater than 5.\n",
        "2. Calculate the mean heading from all headings within the window that passed the above requirements.\n",
        "3. The sum of the initial heading and the mean heading is determined, and using these data, the following variables are estimated:\n",
        "   1. `kappa`: the concentration parameter for a von Mises distribution\n",
        "   2. `mux` and `muy`: the x and y components of `kappa`\n",
        "   3. `wx` and `wy`: the x and y components of wind (the track vector - the heading)\n",
        "4. The `inita`, `mux`, `muy`, `wx`, and `wy` variables are then optimised using log-likelihood and track speed and direction data alongside a constant assumed mean air speed (34.7 m/s).\n",
        "5. The standard deviation of the heading vector perpendicular to the mean direction `yoko` and the standard deviation of the heading vector along the mean direction `tate` are calculated.\n",
        "\n",
        "If convergence is not reached but `tate` can be calculated, the process is repeated until convergence is reached."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 282,
      "metadata": {},
      "outputs": [],
      "source": [
        "time_window = 51\n",
        "fs = (np.timedelta64(60,'s')/np.timedelta64(mode(np.diff(df['DT'])),'s')).astype(int)\n",
        "cutlength = round(45/51 * (fs * 51))\n",
        "expSamp = round(51 * fs)\n",
        "cutv = 4.1667\n",
        "constv = 34.7/3.6\n",
        "error_of_sampling_interval = 5 * fs\n",
        "cutt = (60 * fs) + error_of_sampling_interval\n",
        "winwidth = (time_window * fs) - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run through each datetime value from 1 to end - cutlength. First test if `datetime[i]:datetime[i+51]` is over 51 mins. If yes, then find if `datetime[i]:datetime[i+cutlength]` is over 51 mins. If no, set up a while loop to identify at what index the value is over 51 minutes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 283,
      "metadata": {},
      "outputs": [],
      "source": [
        "# generate a function to create windows capable of running the estimation method. Requirements are 51 minutes of data, with 75% of expected samples\n",
        "def windowFit(DT,start,end,end2,cutt):\n",
        "    if (DT[end] - DT[start]) > np.timedelta64(cutt,'m'):\n",
        "        if (DT[end2 - 1] - DT[start]) < np.timedelta64(cutt,'m'):\n",
        "            while (DT[end2] - DT[start]) < np.timedelta64(cutt,'m'):\n",
        "                end2 = end2 + 1\n",
        "            return range(start,end2)\n",
        "        \n",
        "def findWindows(DT,cutt):\n",
        "    # start from minimum possible point\n",
        "    return list(filter(None,[windowFit(DT,x,x+expSamp,x+cutlength,51) for x in range(0,len(DT) - expSamp)]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Process:\n",
        "\n",
        "1. First identify a suitable window.\n",
        "   1. Determine the start and end such that they are greater than `windwidthsec` apart.\n",
        "   2. Extract only data where the speed is greater than `cutv`, time difference is less than `cutt`, and the heading is not equal to 100 (when direction cannot be calculated i.e. track speed = 0).\n",
        "2. If the resultant window is greater in length than `cutlength`, proceed to maximum likelihood modelling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 284,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "range(185, 230)"
            ]
          },
          "execution_count": 284,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# remove low speed values and where time differences are too long\n",
        "dfRm = df[(df['track_speed'] > cutv) & (df['dt'] < cutt)].reset_index()\n",
        "# find suitable windows\n",
        "windows = findWindows(dfRm['DT'],51)\n",
        "windows[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In R, the function `A1inv` returns a value `k` from input argument `r` such that\n",
        "\n",
        "$A1inv(k) = A1(r)$\n",
        "\n",
        "where \n",
        "\n",
        "$A1(r) = \\frac{I_1(\\kappa)}{I_0(\\kappa)}$\n",
        "\n",
        "where $I_1$ and $I_0$ are the first and zeroth order Bessel functions, respectively.\n",
        "\n",
        "However, Python does not have such a function, but the `circular` package has a [GitHub page](https://github.com/cran/circular) documenting the functions, including `A1inv` and so we can now simply implement this within Python as a new function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 313,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def A1inv(x):\n",
        "    if ((0 <= x) * (x < 0.53)):\n",
        "        return 2 * x + x**3 + (5 * x**5)/6\n",
        "    else:\n",
        "        if (x < 0.85):\n",
        "            return -0.4 + 1.39 * x + 0.43/(1-x)\n",
        "        else:\n",
        "            return 1/(x**3 - 4 * x**2 + 3 * x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The initial method ran through a loop groung through every potential fitting time window. Instead, I think it would be best to optimise the process by first identifying usable windows then running through all those. This would remove the processing time for all none-fitting windows.\n",
        "\n",
        "Suitable windows are ones which pass the following requirements:\n",
        "1. Window length is greater or equal to minimum window size in seconds\n",
        "2. Windows contain sufficient number of data points (75% of expected samples)\n",
        "3. Data in window must be collected at speeds greater than the minimum ground speed (4.1667 m/s)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 314,
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# from tokenize import Double\n",
        "\n",
        "def findWindow(dt,center,windwidthsec):\n",
        "    entr = 0\n",
        "    passesE = False\n",
        "    for qf in range(len(dt) - center):\n",
        "        entr = entr + dt[center + qf]\n",
        "        if entr > windwidthsec:\n",
        "            passesE = True\n",
        "            break\n",
        "    entr = 0\n",
        "    passesS = False\n",
        "    for qb in range(center-1):\n",
        "        entr = entr + dt[center-qb]\n",
        "        if entr > windwidthsec:\n",
        "            passesS = True\n",
        "            break\n",
        "    return passesE * passesS * (passesE - passesS > 44)\n",
        "\n",
        "def trackVectors(id_hd,r,d,index):\n",
        "    rr = []\n",
        "    dd = []\n",
        "    iindex = []\n",
        "    for k in range(len(r)):\n",
        "        if r[k] > cutv:\n",
        "            rr = np.append(rr,r)\n",
        "            dd = np.append(dd,d)\n",
        "            iindex = np.append(iindex,index[k])\n",
        "    inithd_first = id_hd/(3*pi/2)\n",
        "    inita = 0\n",
        "    while inita < 5:\n",
        "        inita = np.abs(np.random.normal(12.5,5))\n",
        "    meangd = np.arctan2(np.sum(np.sin(d)),np.sum(np.cos(d)))\n",
        "    inithd = meangd + inithd_first\n",
        "    initkappa = A1inv(np.mean(np.cos(d - meangd)))\n",
        "    initmux = initkappa * np.cos(inithd)\n",
        "    initmuy = initkappa * np.sin(inithd)\n",
        "    initwx = np.mean(r) * np.cos(meangd) - constv * np.cos(inithd)\n",
        "    initwy = np.mean(r) * np.sin(meangd) - constv * np.sin(inithd)\n",
        "\n",
        "    # answ = sp.optimize.minimize(Likelihoodww(r,d,constv), np.array([inita,initmux,initmuy,initwx,initwy]),method='nelder-mead')\n",
        "\n",
        "    # yoko = Von_Mises_sd(np.sqrt())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 315,
      "metadata": {},
      "outputs": [],
      "source": [
        "rr = rrow[windows[0]]\n",
        "dd = rrow[windows[0]]\n",
        "index = windows[0]\n",
        "max_like = \"NaN\"\n",
        "hd_try = 3\n",
        "\n",
        "# CHANGE THIS TO FOR LOOP\n",
        "id_hd = -hd_try\n",
        "\n",
        "inithd_first = id_hd/(3*np.pi/2)\n",
        "inita = 0\n",
        "while inita < 5:\n",
        "    inita = np.abs(np.random.normal(12.5,5))\n",
        "meangd = np.arctan2(np.sum(np.sin(d)),np.sum(np.cos(d)))\n",
        "inithd = meangd + inithd_first\n",
        "initkappa = A1inv(np.mean(np.cos(d - meangd)))\n",
        "initmux = initkappa * np.cos(inithd)\n",
        "initmuy = initkappa * np.sin(inithd)\n",
        "initwx = np.mean(r) * np.cos(meangd) - constv * np.cos(inithd)\n",
        "initwy = np.mean(r) * np.sin(meangd) - constv * np.sin(inithd)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To speed up calculation, create two versions, one which generates winds every hour or so. Wind should not change so frequently and this may be optimal for more users. Also very useful for trial users."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10.6 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
